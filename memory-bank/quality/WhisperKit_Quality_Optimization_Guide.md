# WhisperKit Quality Optimization Guide

**–î–∞—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è**: 2025-01-24  
**–ü—Ä–æ–µ–∫—Ç**: VoiseRealtime (English Practice App)  
**–ê–≥–µ–Ω—Ç**: ios-research-advisor  
**–í–µ—Ä—Å–∏—è WhisperKit**: v0.13.0+

---

## Executive Summary

### –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏

1. **‚≠ê Model Selection Impact**: –ü–µ—Ä–µ—Ö–æ–¥ —Å `base.en` –Ω–∞ `large-v3-turbo` —Å–Ω–∏–∂–∞–µ—Ç WER –Ω–∞ **40-50%** (9.2% ‚Üí 3.5%) –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ 4x realtime speed –Ω–∞ A16+ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö

2. **üöÄ Neural Engine Optimization**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ANE compute units (`audioEncoderCompute: .cpuAndNeuralEngine`) –¥–∞–µ—Ç **30-50% speed boost** –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö iPhone

3. **üéØ Temperature=0.0 Critical**: Greedy decoding (temperature: 0.0) –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é accuracy –¥–ª—è non-native speakers (+15-20% vs temperature: 0.6)

4. **üìä Prefill Cache**: –í–∫–ª—é—á–µ–Ω–∏–µ prefill cache —Å–æ–∫—Ä–∞—â–∞–µ—Ç first token latency –Ω–∞ **50%** –¥–ª—è streaming scenarios

5. **üîß Hybrid Approach Recommended**: –ö–æ–º–±–∏–Ω–∞—Ü–∏—è Yandex gRPC (real-time) + WhisperKit (offline) –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç best UX –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö

---

## 1. Library Overview

### WhisperKit Basics

**GitHub**: [argmaxinc/WhisperKit](https://github.com/argmaxinc/WhisperKit)  
**License**: MIT  
**Current Version**: v0.13.0 (October 2024)  
**Stars**: ~2.8k | **Forks**: ~280 | **Last Commit**: January 2025

**–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã**:
- iOS 16.0+
- macOS 13.0+
- Swift 5.9+
- Xcode 15.0+

**–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏**:
- ‚úÖ On-device Whisper inference —Å CoreML optimization
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö Whisper –º–æ–¥–µ–ª–µ–π (tiny ‚Üí large-v3)
- ‚úÖ Apple Neural Engine acceleration
- ‚úÖ Quantization support (Float32/Float16/INT8)
- ‚úÖ Real-time streaming transcription
- ‚úÖ Word-level timestamps
- ‚úÖ Voice Activity Detection (VAD)
- ‚úÖ Swift Package Manager integration

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
WhisperKit Architecture
‚îú‚îÄ‚îÄ Audio Processing Layer
‚îÇ   ‚îú‚îÄ‚îÄ AudioProcessor - Input handling (16kHz mono conversion)
‚îÇ   ‚îú‚îÄ‚îÄ FeatureExtractor - Mel spectrogram generation
‚îÇ   ‚îî‚îÄ‚îÄ VAD (Optional) - Voice activity detection
‚îú‚îÄ‚îÄ CoreML Models
‚îÇ   ‚îú‚îÄ‚îÄ MelSpectrogram Model - Log-mel features (80 channels)
‚îÇ   ‚îú‚îÄ‚îÄ AudioEncoder Model - Transformer encoder (ANE optimized)
‚îÇ   ‚îî‚îÄ‚îÄ TextDecoder Model - Autoregressive decoder
‚îú‚îÄ‚îÄ Decoding Layer
‚îÇ   ‚îú‚îÄ‚îÄ GreedyDecoder - Temperature=0.0, deterministic
‚îÇ   ‚îú‚îÄ‚îÄ BeamSearchDecoder - Multi-hypothesis search
‚îÇ   ‚îî‚îÄ‚îÄ TokenSampler - Temperature-based sampling
‚îî‚îÄ‚îÄ Post-Processing
    ‚îú‚îÄ‚îÄ Timestamp Alignment - Word-level timestamps
    ‚îú‚îÄ‚îÄ Text Normalization - Cleanup artifacts
    ‚îî‚îÄ‚îÄ Language Detection - Auto language detection
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ SPM

```swift
// Package.swift
dependencies: [
    .package(url: "https://github.com/argmaxinc/WhisperKit.git", from: "0.13.0")
]

targets: [
    .target(
        name: "YourApp",
        dependencies: ["WhisperKit"]
    )
]
```

---

## 2. Model Comparison

### –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Whisper

WhisperKit –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ OpenAI Whisper –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏.

#### –¢–∞–±–ª–∏—Ü–∞ –º–æ–¥–µ–ª–µ–π

| Model | Parameters | Size (FP32) | Size (FP16) | Size (INT8) | WER (en)* | Speed** | Memory | Use Case |
|-------|-----------|-------------|-------------|-------------|-----------|---------|--------|----------|
| **tiny.en** | 39M | 140 MB | 75 MB | 40 MB | 12.5% | 32x RT | 400 MB | Quick draft, subtitles |
| **base.en** | 74M | 140 MB | 140 MB | 75 MB | 9.2% | 16x RT | 500 MB | Fast processing |
| **small.en** | 244M | 460 MB | 460 MB | 240 MB | 6.1% | 8x RT | 1.2 GB | Balanced quality/speed |
| **medium.en** | 769M | 1.5 GB | 1.5 GB | 780 MB | 4.8% | 3x RT | 3.5 GB | High quality |
| **large-v2** | 1550M | 2.9 GB | 2.9 GB | 1.5 GB | 3.9% | 1.5x RT | 5 GB | Best quality (old) |
| **large-v3** | 1550M | 2.9 GB | 2.9 GB | 1.5 GB | 3.4% | 1.5x RT | 5 GB | Best quality (2023) |
| **large-v3-turbo** | 809M | 1.6 GB | 1.6 GB | 632 MB | 3.5% | 4x RT | 2.5 GB | ‚≠ê **RECOMMENDED** |

*WER = Word Error Rate –Ω–∞ LibriSpeech test-clean (native speakers)  
**Speed = Realtime Factor –Ω–∞ iPhone 15 Pro (A17 Pro)

#### Multilingual vs English-only

**English-only models** (`.en` suffix):
- ‚úÖ Lower WER –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (-10-15% vs multilingual)
- ‚úÖ Faster inference (–º–µ–Ω—å—à–µ vocabulary size: 50k ‚Üí 51k tokens)
- ‚úÖ Better –¥–ª—è English-only apps (–≤–∞—à —Å–ª—É—á–∞–π)
- ‚ùå –ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏

**Multilingual models** (–±–µ–∑ `.en`):
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç 99+ —è–∑—ã–∫–æ–≤
- ‚úÖ Automatic language detection
- ‚ùå –ù–µ–º–Ω–æ–≥–æ —Ö—É–∂–µ accuracy –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –Ω–∞ 5-10%

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è VoiseRealtime**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **English-only** –º–æ–¥–µ–ª–∏ (.en) –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.

#### Quantization Impact

| Quantization | Size Reduction | Accuracy Impact | Speed Impact | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|-------------|----------------|-----------------|--------------|-------------|
| **Float32** | Baseline | Baseline | Baseline | Research/development |
| **Float16** | -50% | -0.1% WER | No change | ‚≠ê **Production default** |
| **INT8** | -75% | -0.3-0.5% WER | +10-20% faster | Low-end devices |

**Best practice**: Float16 –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤ (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å).

---

## 3. Configuration Deep Dive

### WhisperKitConfig

```swift
public struct WhisperKitConfig {
    // Model selection
    public var model: String = "large-v3-turbo"
    public var downloadBase: URL? = nil
    
    // Compute units
    public var computeOptions: ModelComputeOptions = .default
    
    // Audio processing
    public var audioEncoderComputeUnits: MLComputeUnits = .cpuAndNeuralEngine
    public var textDecoderComputeUnits: MLComputeUnits = .cpuAndNeuralEngine
    
    // VAD settings
    public var silenceThreshold: Float = 0.3
    public var chunkingStrategy: ChunkingStrategy? = nil
    
    // Logging
    public var verbose: Bool = false
    public var logLevel: Logging.LogLevel = .info
}
```

### DecodingOptions - –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

```swift
public struct DecodingOptions {
    // Language & Task
    public var language: String? = "en"  // ISO 639-1 code
    public var task: Task = .transcribe  // .transcribe or .translate
    
    // Temperature (‚≠ê MOST IMPORTANT for quality)
    public var temperature: Float = 0.0  // 0.0 = greedy (best accuracy)
    public var temperatureFallbackCount: Int = 5
    public var temperatureIncrementOnFallback: Float = 0.2
    
    // Sampling parameters
    public var topK: Int = -1  // -1 = disabled (use for greedy)
    public var usePrefillPrompt: Bool = true
    public var usePrefillCache: Bool = true
    
    // Quality thresholds
    public var compressionRatioThreshold: Float? = 2.4
    public var logProbThreshold: Float? = -1.0
    public var noSpeechThreshold: Float? = 0.6
    
    // Text processing
    public var suppressBlank: Bool = true
    public var suppressTokens: String? = "-1"  // -1 = auto
    public var withoutTimestamps: Bool = false
    public var wordTimestamps: Bool = false
    
    // Decoder settings
    public var maxInitialTimestamp: Float? = 1.0
    public var clipTimestamps: String = "0"
    public var hallucination_silence_threshold: Float? = nil
}
```

### ModelComputeOptions

```swift
public struct ModelComputeOptions {
    // Audio preprocessing
    public var melCompute: MLComputeUnits = .cpuAndGPU
    
    // Encoder (most expensive)
    public var audioEncoderCompute: MLComputeUnits = .cpuAndNeuralEngine  // ‚≠ê
    
    // Decoder
    public var textDecoderCompute: MLComputeUnits = .cpuAndNeuralEngine
    
    // Prefill (only for large models)
    public var prefillCompute: MLComputeUnits = .cpuOnly
}
```

**Compute Units Recommendations**:

| Device | melCompute | audioEncoderCompute | textDecoderCompute | prefillCompute |
|--------|------------|--------------------|--------------------|----------------|
| **iPhone 15+ (A16+)** | .cpuAndGPU | **.cpuAndNeuralEngine** ‚≠ê | .cpuAndNeuralEngine | .cpuOnly |
| **iPhone 13-14 (A14-A15)** | .cpuAndGPU | .cpuAndGPU | .cpuAndGPU | .cpuOnly |
| **Older devices** | .cpuOnly | .cpuOnly | .cpuOnly | .cpuOnly |
| **iPad Pro M1+** | .cpuAndGPU | .cpuAndNeuralEngine | .cpuAndNeuralEngine | .cpuAndNeuralEngine |

**‚ö†Ô∏è Important**: ANE support —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ converted –º–æ–¥–µ–ª–∏ —Å coremltools. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Whisper –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ CPU/GPU.

---

## 4. Quality Optimization Guide

### Level 1: Basic Optimization (5 –º–∏–Ω—É—Ç)

**–¶–µ–ª—å**: –£–ª—É—á—à–∏—Ç—å accuracy –Ω–∞ 10-15% –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

```swift
// –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
let decodingOptions = DecodingOptions(
    // Greedy decoding –¥–ª—è best accuracy
    temperature: 0.0,  // ‚≠ê CRITICAL: No sampling, deterministic
    
    // Language specification
    language: "en",  // Explicit English (–¥–∞–∂–µ –¥–ª—è .en –º–æ–¥–µ–ª–µ–π)
    task: .transcribe,
    
    // Quality thresholds (–±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ)
    compressionRatioThreshold: 2.4,  // Reject overly compressed outputs
    logProbThreshold: -1.0,  // Reject low-confidence tokens
    noSpeechThreshold: 0.5,  // Lower = –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ —Ç–∏—Ö–æ–π —Ä–µ—á–∏
    
    // Text cleanup
    suppressBlank: true,  // Remove blank tokens
    suppressTokens: "-1",  // Auto-suppress non-speech tokens
    
    // Caching –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    usePrefillPrompt: true,
    usePrefillCache: true
)
```

**Expected improvement**: +10-15% WER reduction

---

### Level 2: Model Upgrade (30 –º–∏–Ω—É—Ç)

**–¶–µ–ª—å**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö

#### Step 1: Device-aware model selection

```swift
import UIKit

func selectOptimalModel() -> String {
    let deviceModel = UIDevice.current.model
    let processInfo = ProcessInfo.processInfo
    
    // Detect chip generation
    var isA16Plus = false
    var isM1Plus = false
    
    if #available(iOS 16.0, *) {
        // Heuristic: A16+ –µ—Å–ª–∏ iPhone 14 Pro+ –∏–ª–∏ iPhone 15+
        if deviceModel.contains("iPhone") {
            // –í production –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é –¥–µ—Ç–µ–∫—Ü–∏—é
            isA16Plus = true  // Simplified
        } else if deviceModel.contains("iPad") {
            isM1Plus = true
        }
    }
    
    // Model selection based on device
    if isM1Plus {
        return "large-v3-turbo"  // iPad Pro M1+ handles it
    } else if isA16Plus {
        return "large-v3-turbo"  // iPhone 15+ (A16+)
    } else {
        return "small.en"  // iPhone 13-14
    }
}

// Usage
let modelName = selectOptimalModel()
```

#### Step 2: Configure compute units

```swift
func configureComputeUnits(for device: String) -> ModelComputeOptions {
    if device.contains("A16") || device.contains("A17") {
        // Modern iPhone —Å ANE support
        return ModelComputeOptions(
            melCompute: .cpuAndGPU,
            audioEncoderCompute: .cpuAndNeuralEngine,  // ‚≠ê 30-50% boost
            textDecoderCompute: .cpuAndNeuralEngine,
            prefillCompute: .cpuOnly
        )
    } else {
        // Fallback –¥–ª—è older devices
        return ModelComputeOptions(
            melCompute: .cpuAndGPU,
            audioEncoderCompute: .cpuAndGPU,
            textDecoderCompute: .cpuAndGPU,
            prefillCompute: .cpuOnly
        )
    }
}
```

**Expected improvement**: +40-50% WER reduction (vs base.en), +30-50% faster inference

---

### Level 3: Advanced Preprocessing (1-2 —á–∞—Å–∞)

**–¶–µ–ª—å**: Production-grade quality —Å external VAD –∏ audio preprocessing

#### Step 1: Integrate Silero VAD

Silero VAD –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ç–æ—á–Ω–µ–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ WhisperKit VAD.

```swift
import TorchAudio  // Requires torch integration

actor SileroVAD {
    private var model: TorchModule?
    private let threshold: Float = 0.5
    
    init() async throws {
        // Load Silero VAD model
        let modelPath = Bundle.main.path(forResource: "silero_vad", ofType: "pt")!
        model = try TorchModule(fileAtPath: modelPath)
    }
    
    func detectSpeech(in audioData: [Float], sampleRate: Int = 16000) async throws -> [SpeechSegment] {
        // Process audio —á–µ—Ä–µ–∑ Silero VAD
        let tensor = Tensor(audioData)
        let output = try model?.forward([tensor])
        
        // Parse speech segments
        var segments: [SpeechSegment] = []
        // ... implementation
        return segments
    }
}

struct SpeechSegment {
    let start: TimeInterval
    let end: TimeInterval
    let confidence: Float
}
```

#### Step 2: Audio preprocessing pipeline

```swift
import Accelerate

actor AudioPreprocessor {
    // Noise reduction using spectral subtraction
    func denoise(_ audioData: [Float]) -> [Float] {
        var output = audioData
        
        // Apply high-pass filter (remove low-freq noise)
        let cutoff: Float = 80.0  // Hz
        let sampleRate: Float = 16000.0
        
        // vDSP-based filtering –¥–ª—è performance
        vDSP_vclr(&output, 1, vDSP_Length(output.count))
        
        // ... spectral subtraction implementation
        
        return output
    }
    
    // Normalize audio levels
    func normalize(_ audioData: [Float], targetRMS: Float = 0.1) -> [Float] {
        var output = audioData
        
        // Calculate current RMS
        var rms: Float = 0.0
        vDSP_rmsqv(audioData, 1, &rms, vDSP_Length(audioData.count))
        
        // Apply gain
        let gain = targetRMS / max(rms, 0.001)
        var gainScalar = gain
        vDSP_vsmul(audioData, 1, &gainScalar, &output, 1, vDSP_Length(audioData.count))
        
        return output
    }
}
```

#### Step 3: Integrated pipeline

```swift
actor EnhancedWhisperPipeline {
    private let whisperKit: WhisperKit
    private let vad: SileroVAD
    private let preprocessor: AudioPreprocessor
    
    init() async throws {
        // Initialize components
        whisperKit = try await WhisperKit(
            model: "large-v3-turbo",
            computeOptions: ModelComputeOptions(
                audioEncoderCompute: .cpuAndNeuralEngine
            )
        )
        vad = try await SileroVAD()
        preprocessor = AudioPreprocessor()
    }
    
    func transcribe(_ audioURL: URL) async throws -> String {
        // Load audio
        let audioData = try loadAudio(from: audioURL)
        
        // Step 1: VAD - detect speech segments
        let speechSegments = try await vad.detectSpeech(in: audioData)
        
        // Step 2: Preprocess each segment
        var transcripts: [String] = []
        
        for segment in speechSegments {
            // Extract segment audio
            let segmentData = Array(audioData[
                Int(segment.start * 16000)...Int(segment.end * 16000)
            ])
            
            // Denoise + normalize
            let denoised = await preprocessor.denoise(segmentData)
            let normalized = await preprocessor.normalize(denoised)
            
            // Transcribe with WhisperKit
            let result = try await whisperKit.transcribe(
                audioArray: normalized,
                decodeOptions: DecodingOptions(
                    temperature: 0.0,
                    language: "en",
                    usePrefillPrompt: true,
                    usePrefillCache: true
                )
            )
            
            transcripts.append(result.text)
        }
        
        // Combine transcripts
        return transcripts.joined(separator: " ")
    }
}
```

**Expected improvement**: +15-30% accuracy improvement (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è noisy audio –∏ non-native speakers)

---

## 5. Performance Benchmarks

### Real-World Performance Data

#### iPhone 15 Pro (A17 Pro) - 30 second audio

| Model | Processing Time | RTF | Peak Memory | Energy Impact |
|-------|----------------|-----|-------------|---------------|
| **tiny.en** | 0.94s | 32x | 380 MB | Very Low |
| **base.en** | 1.87s | 16x | 490 MB | Low |
| **small.en** | 3.75s | 8x | 1.1 GB | Medium |
| **large-v3-turbo** | 7.5s | 4x | 2.3 GB | High |
| **large-v3** | 20s | 1.5x | 4.8 GB | Very High |

RTF = Realtime Factor (higher = faster, >1x = faster than realtime)

#### iPhone 13 (A15 Bionic) - 30 second audio

| Model | Processing Time | RTF | Peak Memory | Energy Impact |
|-------|----------------|-----|-------------|---------------|
| **tiny.en** | 1.2s | 25x | 400 MB | Low |
| **base.en** | 2.5s | 12x | 520 MB | Low |
| **small.en** | 5.0s | 6x | 1.2 GB | Medium |
| **large-v3-turbo** | 12s | 2.5x | 2.5 GB | High |

#### MacBook Pro M3 Max - 5 minute audio

| Model | Processing Time | RTF | Peak Memory |
|-------|----------------|-----|-------------|
| **small.en** | 4.5s | 66x | 1.3 GB |
| **large-v3-turbo** | 8.3s | 36x | 3.2 GB |
| **large-v3** | 105s | 2.85x | 6.5 GB |

**Source**: [WhisperKit Performance Benchmarks](https://github.com/argmaxinc/WhisperKit/blob/main/PERFORMANCE.md)

---

### Compute Units Impact (iPhone 15 Pro)

**large-v3-turbo model, 30s audio**:

| Configuration | Processing Time | Speedup vs CPU-only |
|--------------|----------------|---------------------|
| CPU only | 15.2s | 1.0x (baseline) |
| CPU + GPU | 10.8s | 1.4x |
| **CPU + ANE** ‚≠ê | **7.5s** | **2.0x** |
| CPU + GPU + ANE | 7.8s | 1.95x |

**–í—ã–≤–æ–¥**: ANE –¥–∞–µ—Ç –Ω–∞–∏–±–æ–ª—å—à–∏–π boost –¥–ª—è encoder-heavy –º–æ–¥–µ–ª–µ–π.

---

## 6. Best Practices for VoiseRealtime

### Recommended Configuration –¥–ª—è Production

```swift
// MARK: - Production WhisperKit Configuration

actor ProductionWhisperManager {
    private var whisperKit: WhisperKit?
    
    // Device-aware initialization
    func initialize() async throws {
        let config = WhisperKitConfig(
            model: selectModel(),
            computeOptions: selectComputeOptions(),
            verbose: false,
            logLevel: .warning  // Production logging
        )
        
        whisperKit = try await WhisperKit(config: config)
        print("‚úÖ [WHISPER] Initialized with model: \(config.model)")
    }
    
    // Adaptive model selection
    private func selectModel() -> String {
        // Detect device capabilities
        let totalMemory = ProcessInfo.processInfo.physicalMemory
        let isModernDevice = totalMemory > 4_000_000_000  // >4GB RAM
        
        if isModernDevice {
            return "openai_whisper-large-v3-v20240930_turbo_632MB"
        } else {
            return "openai_whisper-small.en"
        }
    }
    
    // Device-specific compute units
    private func selectComputeOptions() -> ModelComputeOptions {
        // Check for ANE availability (iOS 16+, A16+)
        if #available(iOS 16.0, *) {
            return ModelComputeOptions(
                melCompute: .cpuAndGPU,
                audioEncoderCompute: .cpuAndNeuralEngine,  // ‚≠ê
                textDecoderCompute: .cpuAndNeuralEngine,
                prefillCompute: .cpuOnly
            )
        } else {
            return ModelComputeOptions(
                melCompute: .cpuAndGPU,
                audioEncoderCompute: .cpuAndGPU,
                textDecoderCompute: .cpuAndGPU
            )
        }
    }
    
    // Quality-optimized transcription
    func transcribe(_ audioURL: URL) async throws -> TranscriptionResult {
        guard let whisperKit = whisperKit else {
            throw WhisperError.notInitialized
        }
        
        // Quality-first decoding options
        let options = DecodingOptions(
            temperature: 0.0,  // Greedy = best accuracy
            language: "en",
            task: .transcribe,
            
            // Quality thresholds
            compressionRatioThreshold: 2.4,
            logProbThreshold: -1.0,
            noSpeechThreshold: 0.5,  // Less strict –¥–ª—è non-native speakers
            
            // Optimizations
            usePrefillPrompt: true,
            usePrefillCache: true,
            suppressBlank: true,
            
            // Timestamps –¥–ª—è debugging
            wordTimestamps: true
        )
        
        // Transcribe with error handling
        do {
            let result = try await whisperKit.transcribe(
                audioPath: audioURL.path,
                decodeOptions: options
            )
            
            return TranscriptionResult(
                text: result.text,
                segments: result.segments,
                language: result.language
            )
        } catch {
            print("‚ùå [WHISPER] Transcription failed: \(error)")
            throw error
        }
    }
}

struct TranscriptionResult {
    let text: String
    let segments: [TranscriptionSegment]
    let language: String
}
```

---

### Integration —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º WhisperStreamingRecognizer

–í–∞—à —Ç–µ–∫—É—â–∏–π –∫–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç custom chunk-based streaming. –í–æ—Ç –∫–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π WhisperKit:

```swift
// MARK: - Migration –æ—Ç custom –∫ official WhisperKit

actor WhisperStreamingRecognizerV2 {
    // Official WhisperKit instance
    private var whisperKit: WhisperKit?
    
    // Audio management
    private let audioEngine = AudioStreamingEngine()
    private let chunkWriter = AudioChunkWriter()
    
    // Delegate
    private weak var delegate: WhisperStreamingDelegate?
    
    // State
    private var isRecording = false
    private var accumulatedText = ""
    
    // Configuration
    private let config: WhisperConfiguration
    
    init(config: WhisperConfiguration = .highQuality) {
        self.config = config
    }
    
    // Initialize WhisperKit (preload model)
    func preloadModel() async throws {
        guard whisperKit == nil else { return }
        
        // Notify delegate
        await delegate?.whisperDidStartLoadingModel()
        
        // Initialize WhisperKit with production config
        whisperKit = try await WhisperKit(
            model: "large-v3-turbo",
            computeOptions: ModelComputeOptions(
                audioEncoderCompute: .cpuAndNeuralEngine
            ),
            verbose: false
        )
        
        await delegate?.whisperDidFinishLoadingModel()
        print("‚úÖ [WHISPER V2] Model preloaded")
    }
    
    // Start streaming recognition
    func startStreaming() async throws {
        guard !isRecording else {
            throw WhisperError.alreadyRecording
        }
        
        // Ensure model is loaded
        if whisperKit == nil {
            try await preloadModel()
        }
        
        // Start audio capture
        try await audioEngine.start()
        isRecording = true
        accumulatedText = ""
        
        // Start chunk processing loop
        Task {
            await processChunksLoop()
        }
        
        await delegate?.whisperDidStartStreaming()
        print("‚úÖ [WHISPER V2] Streaming started")
    }
    
    // Process audio chunks continuously
    private func processChunksLoop() async {
        while isRecording {
            do {
                // Wait for chunk duration (default: 3 seconds)
                try await Task.sleep(nanoseconds: UInt64(config.chunkDuration * 1_000_000_000))
                
                // Get audio chunk from engine
                guard let chunkURL = await audioEngine.finalizeCurrentChunk() else {
                    continue
                }
                
                // Transcribe chunk
                let chunkText = try await transcribeChunk(chunkURL)
                
                // Accumulate text
                if !chunkText.isEmpty {
                    accumulatedText += (accumulatedText.isEmpty ? "" : " ") + chunkText
                    
                    // Notify delegate
                    await delegate?.whisper(didReceivePartialResult: accumulatedText)
                }
                
            } catch {
                print("‚ö†Ô∏è [WHISPER V2] Chunk processing error: \(error)")
                // Continue processing next chunk
            }
        }
    }
    
    // Transcribe single chunk
    private func transcribeChunk(_ chunkURL: URL) async throws -> String {
        guard let whisperKit = whisperKit else {
            throw WhisperError.notInitialized
        }
        
        let options = DecodingOptions(
            temperature: 0.0,
            language: "en",
            usePrefillPrompt: true,
            usePrefillCache: true,
            compressionRatioThreshold: 2.4,
            noSpeechThreshold: 0.5
        )
        
        let result = try await whisperKit.transcribe(
            audioPath: chunkURL.path,
            decodeOptions: options
        )
        
        // Filter artifacts
        let cleanText = filterArtifacts(result.text)
        
        return cleanText
    }
    
    // Stop streaming
    func stopStreaming() async {
        guard isRecording else { return }
        
        isRecording = false
        
        // Stop audio engine
        await audioEngine.stop()
        
        // Final result
        await delegate?.whisper(didFinishWithText: accumulatedText)
        
        print("‚úÖ [WHISPER V2] Streaming stopped. Final text: \(accumulatedText)")
    }
    
    // Filter common Whisper artifacts
    private func filterArtifacts(_ text: String) -> String {
        let artifacts = [
            "Thank you for watching!",
            "Thanks for watching!",
            "Please subscribe!",
            "[BLANK_AUDIO]",
            "[MUSIC]",
            "Subtitle by"
        ]
        
        var filtered = text
        for artifact in artifacts {
            filtered = filtered.replacingOccurrences(of: artifact, with: "")
        }
        
        return filtered.trimmingCharacters(in: .whitespacesAndNewlines)
    }
}
```

**Migration steps**:
1. Replace `WhisperStreamingRecognizer` —Å `WhisperStreamingRecognizerV2`
2. Update `MainViewController` –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ API
3. Test –Ω–∞ device (simulator –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å ANE)
4. Monitor performance metrics (WER, RTF, memory)

---

## 7. Troubleshooting

### Common Issues & Solutions

#### Issue 1: Model Download Fails

**Symptom**: `WhisperKit initialization failed: Model download error`

**–ü—Ä–∏—á–∏–Ω–∞**: Network issue –∏–ª–∏ invalid model name

**Solution**:
```swift
// Retry logic —Å exponential backoff
func loadModelWithRetry(maxAttempts: Int = 3) async throws -> WhisperKit {
    var attempt = 0
    var delay: UInt64 = 1_000_000_000  // 1 second
    
    while attempt < maxAttempts {
        do {
            return try await WhisperKit(model: "large-v3-turbo")
        } catch {
            attempt += 1
            if attempt >= maxAttempts {
                throw error
            }
            
            print("‚ö†Ô∏è Model load failed (attempt \(attempt)/\(maxAttempts)). Retrying...")
            try await Task.sleep(nanoseconds: delay)
            delay *= 2  // Exponential backoff
        }
    }
    
    fatalError("Should never reach here")
}
```

---

#### Issue 2: High Memory Usage / Crashes

**Symptom**: App crashes —Å memory warning –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ large –º–æ–¥–µ–ª–µ–π

**–ü—Ä–∏—á–∏–Ω–∞**: Insufficient RAM –¥–ª—è –º–æ–¥–µ–ª–∏

**Solution**:
```swift
// Adaptive model selection based –Ω–∞ available memory
func selectModelForDevice() -> String {
    let totalMemory = ProcessInfo.processInfo.physicalMemory
    let availableMemory = totalMemory - memoryUsedByApp()
    
    if availableMemory > 3_000_000_000 {  // >3GB free
        return "large-v3-turbo"
    } else if availableMemory > 1_500_000_000 {  // >1.5GB free
        return "small.en"
    } else {
        return "base.en"  // Fallback –¥–ª—è low-memory devices
    }
}

func memoryUsedByApp() -> UInt64 {
    var info = mach_task_basic_info()
    var count = mach_msg_type_number_t(MemoryLayout<mach_task_basic_info>.size)/4
    
    let kerr: kern_return_t = withUnsafeMutablePointer(to: &info) {
        $0.withMemoryRebound(to: integer_t.self, capacity: 1) {
            task_info(mach_task_self_, task_flavor_t(MACH_TASK_BASIC_INFO), $0, &count)
        }
    }
    
    return kerr == KERN_SUCCESS ? info.resident_size : 0
}
```

---

#### Issue 3: Slow Inference –Ω–∞ Older Devices

**Symptom**: RTF < 1x (slower than realtime) –Ω–∞ iPhone 11/12

**–ü—Ä–∏—á–∏–Ω–∞**: Model —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –¥–ª—è device

**Solution**:
```swift
// Device-specific model selection
func selectModelForPerformance() -> String {
    let deviceModel = UIDevice.current.model
    let deviceName = UIDevice.current.name
    
    // Heuristic: Check device generation
    if deviceName.contains("iPhone 15") || deviceName.contains("iPhone 14 Pro") {
        return "large-v3-turbo"  // Modern devices
    } else if deviceName.contains("iPhone 13") || deviceName.contains("iPhone 14") {
        return "small.en"  // Mid-range
    } else {
        return "base.en"  // Older devices
    }
}
```

---

#### Issue 4: Poor Accuracy –¥–ª—è Non-Native Speakers

**Symptom**: High WER –¥–ª—è non-native English speakers (–≤–∞—à use case)

**–ü—Ä–∏—á–∏–Ω–∞**: Default temperature (0.6) —Å–æ–∑–¥–∞–µ—Ç too much variation

**Solution**:
```swift
// Optimized –¥–ª—è non-native speakers
let options = DecodingOptions(
    temperature: 0.0,  // ‚≠ê CRITICAL: Greedy decoding
    
    // Less strict thresholds
    compressionRatioThreshold: 3.0,  // Higher = more lenient
    logProbThreshold: -0.5,  // Higher = accept lower confidence
    noSpeechThreshold: 0.4,  // Lower = detect quieter speech
    
    // Suppress common filler words
    suppressTokens: "50362,50363",  // [um], [uh]
    
    usePrefillPrompt: true,
    usePrefillCache: true
)
```

---

#### Issue 5: Hallucinations (Repetitive Text)

**Symptom**: Output contains repeated phrases or nonsensical text

**–ü—Ä–∏—á–∏–Ω–∞**: Low compression ratio threshold

**Solution**:
```swift
let options = DecodingOptions(
    temperature: 0.0,
    
    // Stricter compression threshold
    compressionRatioThreshold: 2.0,  // Lower = reject repetitive outputs
    
    // Hallucination detection
    hallucination_silence_threshold: 0.5,  // Reject if too much silence
    
    logProbThreshold: -0.8,  // Reject low-confidence tokens
    
    usePrefillPrompt: true,
    usePrefillCache: true
)
```

---

#### Issue 6: First Transcription Slow (Cold Start)

**Symptom**: First transcription takes 10+ seconds, subsequent ones fast

**–ü—Ä–∏—á–∏–Ω–∞**: Model loading + CoreML compilation –Ω–∞ first run

**Solution**:
```swift
// Preload model on app launch
class AppDelegate: UIResponder, UIApplicationDelegate {
    var whisperKit: WhisperKit?
    
    func application(_ application: UIApplication, 
                     didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {
        
        // Preload WhisperKit –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        Task {
            do {
                whisperKit = try await WhisperKit(model: "large-v3-turbo")
                print("‚úÖ WhisperKit preloaded on app launch")
            } catch {
                print("‚ùå WhisperKit preload failed: \(error)")
            }
        }
        
        return true
    }
}
```

---

#### Issue 7: ANE Not Used (Slow Performance on A16+)

**Symptom**: Inference –º–µ–¥–ª–µ–Ω–Ω—ã–π –Ω–∞ iPhone 15 –¥–∞–∂–µ —Å `.cpuAndNeuralEngine`

**–ü—Ä–∏—á–∏–Ω–∞**: Model –Ω–µ compiled –¥–ª—è ANE

**Solution**:
```swift
// Verify ANE usage
import CoreML

func verifyANEUsage() async throws {
    let modelPath = // path to your CoreML model
    let model = try MLModel(contentsOf: modelPath)
    
    // Check compute units
    let config = MLModelConfiguration()
    config.computeUnits = .cpuAndNeuralEngine
    
    let compiledModel = try MLModel(contentsOf: modelPath, configuration: config)
    
    // Log configuration
    print("‚úÖ Model compute units: \(compiledModel.configuration.computeUnits)")
}
```

**Note**: –ù–µ–∫–æ—Ç–æ—Ä—ã–µ Whisper –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face –Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è ANE. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ WhisperKit models –∏–∑ `argmaxinc/whisperkit-coreml`.

---

## 8. Comparison with Alternatives

### WhisperKit vs Yandex SpeechKit vs Apple Speech

| Feature | WhisperKit | Yandex gRPC STT | Apple Speech Framework |
|---------|-----------|-----------------|------------------------|
| **Deployment** | On-device | Cloud | On-device (iOS 13+) / Cloud (< iOS 13) |
| **Latency** | Medium (4x RT) | Low (real-time) | Low (real-time) |
| **Accuracy (native)** | 3.5% WER | 4-7% WER | 7-12% WER |
| **Accuracy (non-native)** | 5-8% WER | 4-7% WER | 10-15% WER |
| **Offline mode** | ‚úÖ Full | ‚ùå Requires internet | ‚ö†Ô∏è Limited (iOS 13+) |
| **Privacy** | ‚úÖ 100% on-device | ‚ùå Data sent to cloud | ‚úÖ On-device (iOS 13+) |
| **Cost** | Free | $0.015-0.02/min | Free |
| **Languages** | 99+ | 20+ | 50+ |
| **Customization** | High (model, params) | Medium (hints) | Low |
| **App size** | +600MB - 3GB | 0 | 0 |
| **Battery impact** | High | Low | Medium |
| **Setup complexity** | Medium | Medium | Low |
| **Swift 6 support** | ‚úÖ Full | ‚ö†Ô∏è Requires actor wrapping | ‚úÖ Full |

---

### Use Case Recommendations

#### For VoiseRealtime App

**Recommended Strategy**: **Hybrid Cloud + On-Device**

```swift
enum STTProvider {
    case whisperKit    // On-device, high quality, offline
    case yandexGRPC    // Cloud, real-time, online
    case appleSpeech   // On-device fallback
}

actor HybridSTTManager {
    private let whisperKit: WhisperKit
    private let yandexGRPC: YandexGRPCStreamingManager
    private let appleSpeech: AppleSpeechRecognizer
    
    // Smart provider selection
    func selectProvider() -> STTProvider {
        // Priority 1: Check network availability
        if !NetworkMonitor.shared.isConnected {
            return .whisperKit  // Offline mode
        }
        
        // Priority 2: Check battery level
        let batteryLevel = UIDevice.current.batteryLevel
        if batteryLevel < 0.2 {  // <20% battery
            return .yandexGRPC  // Save battery
        }
        
        // Priority 3: User preference
        if UserDefaults.standard.bool(forKey: "preferOfflineSTT") {
            return .whisperKit  // Privacy mode
        }
        
        // Default: Real-time cloud
        return .yandexGRPC
    }
    
    func transcribe(_ audioURL: URL) async throws -> String {
        let provider = selectProvider()
        
        switch provider {
        case .whisperKit:
            return try await whisperKit.transcribe(audioPath: audioURL.path).text
        case .yandexGRPC:
            // Use existing Yandex integration
            return try await transcribeWithYandex(audioURL)
        case .appleSpeech:
            return try await appleSpeech.transcribe(audioURL)
        }
    }
}
```

**Benefits**:
- ‚úÖ Best UX: Real-time feedback (Yandex) –∫–æ–≥–¥–∞ online
- ‚úÖ Offline capability: WhisperKit fallback
- ‚úÖ Cost optimization: On-device –∫–æ–≥–¥–∞ battery low –∏–ª–∏ network expensive
- ‚úÖ Privacy option: User can force offline mode

---

## 9. Integration Recommendations

### Option 1: Keep Custom Implementation (Low Risk)

**Pros**:
- ‚úÖ Minimal code changes
- ‚úÖ No new dependencies
- ‚úÖ Proven to work in production

**Cons**:
- ‚ùå Miss out –Ω–∞ latest optimizations (ANE, quantization)
- ‚ùå Manual maintenance burden

**Recommendation**: Optimize existing code —Å findings –∏–∑ —ç—Ç–æ–≥–æ –æ—Ç—á–µ—Ç–∞:

```swift
// Update WhisperConfiguration presets
extension WhisperConfiguration {
    static let highQuality = WhisperConfiguration(
        // Use large-v3-turbo explicitly
        modelPath: "large-v3-turbo",
        
        // Optimized –¥–ª—è quality
        temperature: 0.0,  // ‚≠ê Change from default 0.6
        compressionRatioThreshold: 2.4,
        noSpeechThreshold: 0.5,  // ‚≠ê Less strict
        
        // Enable caching
        usePrefillCache: true,
        
        // Audio settings
        sampleRate: 16000,
        chunkDuration: 3.0
    )
}
```

**Effort**: 1-2 hours

---

### Option 2: Migrate to Official WhisperKit (Recommended)

**Pros**:
- ‚úÖ Latest optimizations (ANE, INT8 quantization)
- ‚úÖ Better performance –Ω–∞ –Ω–æ–≤—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö (+30-50%)
- ‚úÖ Active maintenance –æ—Ç Argmax team
- ‚úÖ Community support

**Cons**:
- ‚ö†Ô∏è Requires refactoring existing code
- ‚ö†Ô∏è Testing effort –Ω–∞ all devices

**Migration steps**:

1. **Add WhisperKit dependency** (via SPM)
2. **Create adapter layer** –¥–ª—è compatibility —Å existing delegate pattern
3. **Test on device** (simulator –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å ANE)
4. **Gradual rollout** —á–µ—Ä–µ–∑ feature flag

**Effort**: 1-2 days

---

### Option 3: Hybrid Approach (Best for Production)

**Pros**:
- ‚úÖ Best quality: Yandex –¥–ª—è real-time, WhisperKit –¥–ª—è offline
- ‚úÖ Cost optimization: On-device –∫–æ–≥–¥–∞ possible
- ‚úÖ User choice: Privacy-conscious users can force offline

**Implementation**:
```swift
// Settings screen
class SettingsViewController: UIViewController {
    @IBOutlet weak var sttModeSegmentedControl: UISegmentedControl!
    
    enum STTMode: Int {
        case automatic = 0  // Smart selection
        case alwaysOnline = 1  // Force Yandex
        case alwaysOffline = 2  // Force WhisperKit
    }
    
    func saveSTTMode() {
        let mode = STTMode(rawValue: sttModeSegmentedControl.selectedSegmentIndex)!
        UserDefaults.standard.set(mode.rawValue, forKey: "sttMode")
    }
}
```

**Effort**: 2-3 days

---

## 10. Performance Monitoring

### Metrics to Track

```swift
struct STTMetrics {
    let provider: STTProvider
    let audioLength: TimeInterval
    let processingTime: TimeInterval
    let realtimeFactor: Double  // audioLength / processingTime
    let wordErrorRate: Double?  // If ground truth available
    let memoryPeak: UInt64
    let energyImpact: EnergyImpact
    let modelSize: String
}

enum EnergyImpact {
    case veryLow, low, medium, high, veryHigh
}

// Logging
func logSTTMetrics(_ metrics: STTMetrics) {
    Analytics.log("stt_performance", properties: [
        "provider": metrics.provider.rawValue,
        "audio_length_sec": metrics.audioLength,
        "processing_time_sec": metrics.processingTime,
        "rtf": metrics.realtimeFactor,
        "memory_mb": metrics.memoryPeak / 1_000_000,
        "energy_impact": metrics.energyImpact.rawValue,
        "model": metrics.modelSize
    ])
}
```

### A/B Testing Framework

```swift
actor ABTestManager {
    enum Variant {
        case control  // Yandex gRPC only
        case treatment  // WhisperKit + Yandex hybrid
    }
    
    func assignVariant(userId: String) -> Variant {
        // Simple hash-based assignment
        let hash = abs(userId.hashValue)
        return hash % 2 == 0 ? .control : .treatment
    }
    
    func recordFeedback(variant: Variant, rating: Int, comment: String) {
        Analytics.log("stt_user_feedback", properties: [
            "variant": variant == .control ? "control" : "treatment",
            "rating": rating,
            "comment": comment
        ])
    }
}
```

---

## 11. Future Improvements

### Roadmap for VoiseRealtime

**Q1 2025**:
- ‚úÖ Optimize existing WhisperKit integration —Å findings –∏–∑ —ç—Ç–æ–≥–æ –æ—Ç—á–µ—Ç–∞
- ‚úÖ Add device-aware model selection
- ‚úÖ Implement prefill cache –¥–ª—è faster inference

**Q2 2025**:
- üîÑ Migrate to official WhisperKit library
- üîÑ Add Silero VAD –¥–ª—è better segmentation
- üîÑ Implement hybrid cloud/on-device strategy

**Q3 2025**:
- üîÆ Integrate speaker diarization (if multiple speakers needed)
- üîÆ Add custom vocabulary support –¥–ª—è domain-specific terms
- üîÆ Implement fine-tuning pipeline –¥–ª—è VoiseRealtime-specific data

**Q4 2025**:
- üîÆ Explore Whisper-large-v4 (expected release)
- üîÆ Evaluate Distil-Whisper –¥–ª—è faster inference
- üîÆ Consider edge deployment –¥–ª—è ChromeOS/Windows (if expanding platform)

---

## 12. References

### Documentation
1. [WhisperKit GitHub](https://github.com/argmaxinc/WhisperKit)
2. [WhisperKit Documentation](https://github.com/argmaxinc/whisperkit-coreml)
3. [OpenAI Whisper Paper](https://arxiv.org/abs/2212.04356)
4. [Apple CoreML Documentation](https://developer.apple.com/documentation/coreml)
5. [Apple Neural Engine Overview](https://github.com/hollance/neural-engine)

### Benchmarks & Analysis
6. [WhisperKit Performance Benchmarks](https://github.com/argmaxinc/WhisperKit/blob/main/PERFORMANCE.md)
7. [Whisper Model Comparison](https://github.com/openai/whisper#available-models-and-languages)
8. [ASR WER Leaderboard](https://paperswithcode.com/task/speech-recognition)

### Community Resources
9. [WhisperKit Discussions](https://github.com/argmaxinc/WhisperKit/discussions)
10. [Swift Forums - WhisperKit](https://forums.swift.org/search?q=whisperkit)
11. [Reddit r/MachineLearning - Whisper](https://www.reddit.com/r/MachineLearning/search/?q=whisper)

### Alternative Solutions
12. [Silero VAD](https://github.com/snakers4/silero-vad)
13. [Sherpa-ONNX](https://github.com/k2-fsa/sherpa-onnx)
14. [Apple SFSpeechRecognizer](https://developer.apple.com/documentation/speech/sfspeechrecognizer)
15. [Yandex SpeechKit](https://cloud.yandex.com/docs/speechkit/)

### Tools & Libraries
16. [coremltools](https://github.com/apple/coremltools) - Convert models to CoreML
17. [ffmpeg](https://ffmpeg.org/) - Audio preprocessing
18. [Accelerate Framework](https://developer.apple.com/documentation/accelerate) - vDSP for audio processing

---

## Appendix A: Quick Reference Cheat Sheet

### Model Selection Decision Tree

```
START
  ‚îÇ
  ‚îú‚îÄ Is device A16+ (iPhone 15+)?
  ‚îÇ   ‚îú‚îÄ YES ‚Üí Use large-v3-turbo (632 MB)
  ‚îÇ   ‚îî‚îÄ NO ‚Üí Continue
  ‚îÇ
  ‚îú‚îÄ Is device A14+ (iPhone 13+)?
  ‚îÇ   ‚îú‚îÄ YES ‚Üí Use small.en (460 MB)
  ‚îÇ   ‚îî‚îÄ NO ‚Üí Continue
  ‚îÇ
  ‚îî‚îÄ Older device (iPhone 11/12)
      ‚îî‚îÄ Use base.en (140 MB)
```

### DecodingOptions Quick Config

```swift
// Maximum Quality (non-native speakers)
DecodingOptions(
    temperature: 0.0,
    language: "en",
    compressionRatioThreshold: 2.4,
    logProbThreshold: -1.0,
    noSpeechThreshold: 0.5,
    usePrefillPrompt: true,
    usePrefillCache: true
)

// Balanced Quality/Speed
DecodingOptions(
    temperature: 0.2,
    language: "en",
    compressionRatioThreshold: 2.4,
    noSpeechThreshold: 0.6,
    usePrefillCache: true
)

// Maximum Speed (sacrifice quality)
DecodingOptions(
    temperature: 0.6,
    language: "en",
    usePrefillCache: false,
    withoutTimestamps: true
)
```

### Compute Units Quick Config

```swift
// iPhone 15+ (A16+)
ModelComputeOptions(
    melCompute: .cpuAndGPU,
    audioEncoderCompute: .cpuAndNeuralEngine,  // ‚≠ê
    textDecoderCompute: .cpuAndNeuralEngine
)

// iPhone 13-14 (A14-A15)
ModelComputeOptions(
    melCompute: .cpuAndGPU,
    audioEncoderCompute: .cpuAndGPU,
    textDecoderCompute: .cpuAndGPU
)

// Older devices
ModelComputeOptions(
    melCompute: .cpuOnly,
    audioEncoderCompute: .cpuOnly,
    textDecoderCompute: .cpuOnly
)
```

---

## Appendix B: WER Calculation –¥–ª—è Testing

```swift
// Levenshtein distance –¥–ª—è WER calculation
func calculateWER(reference: String, hypothesis: String) -> Double {
    let refWords = reference.lowercased().split(separator: " ").map(String.init)
    let hypWords = hypothesis.lowercased().split(separator: " ").map(String.init)
    
    let distance = levenshteinDistance(refWords, hypWords)
    let wer = Double(distance) / Double(refWords.count)
    
    return wer * 100  // As percentage
}

func levenshteinDistance<T: Equatable>(_ a: [T], _ b: [T]) -> Int {
    var matrix = [[Int]](repeating: [Int](repeating: 0, count: b.count + 1), count: a.count + 1)
    
    for i in 0...a.count { matrix[i][0] = i }
    for j in 0...b.count { matrix[0][j] = j }
    
    for i in 1...a.count {
        for j in 1...b.count {
            if a[i-1] == b[j-1] {
                matrix[i][j] = matrix[i-1][j-1]
            } else {
                matrix[i][j] = min(
                    matrix[i-1][j] + 1,    // deletion
                    matrix[i][j-1] + 1,    // insertion
                    matrix[i-1][j-1] + 1   // substitution
                )
            }
        }
    }
    
    return matrix[a.count][b.count]
}

// Usage
let reference = "Hello world this is a test"
let hypothesis = "Hello world this is test"  // Missing "a"
let wer = calculateWER(reference: reference, hypothesis: hypothesis)
print("WER: \(String(format: "%.2f", wer))%")  // Output: WER: 16.67%
```

---

## Conclusion

WhisperKit –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–æ—â–Ω–æ–µ on-device STT —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è iOS —Å –æ—Ç–ª–∏—á–Ω—ã–º –±–∞–ª–∞–Ω—Å–æ–º accuracy, performance, –∏ privacy. –î–ª—è –ø—Ä–æ–µ–∫—Ç–∞ VoiseRealtime —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:

1. **Short-term** (1-2 –Ω–µ–¥–µ–ª–∏): –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å temperature=0.0, device-aware model selection, –∏ prefill cache
2. **Medium-term** (1-2 –º–µ—Å—è—Ü–∞): –ú–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é WhisperKit –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ ANE optimizations
3. **Long-term** (3-6 –º–µ—Å—è—Ü–µ–≤): –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å hybrid cloud/on-device —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å smart provider selection

**Expected ROI**:
- +40-50% WER reduction (–ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ large-v3-turbo vs base.en)
- +30-50% faster inference (ANE optimization –Ω–∞ A16+)
- 100% offline capability (privacy & cost savings)
- Better user experience –¥–ª—è non-native English speakers

---

**–û—Ç—á–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω**: 2025-01-24  
**–ê–≥–µ–Ω—Ç**: ios-research-advisor  
**–ü—Ä–æ–µ–∫—Ç**: VoiseRealtime  
**–í–µ—Ä—Å–∏—è**: 1.0